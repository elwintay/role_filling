{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import LongformerTokenizer, LongformerModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datapath):\n",
    "    data = []\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_class_data(data):\n",
    "    class_data = pd.DataFrame(columns=['DocID','Text','Attack','Kidnapping',\n",
    "                                       'Bombing','Robbery','Arson','Forced'])\n",
    "    class_dict = {'attack': 0,\n",
    "                 'kidnapping': 1,\n",
    "                 'bombing': 2,\n",
    "                 'robbery': 3,\n",
    "                 'arson': 4,\n",
    "                 'forced work stoppage': 5}\n",
    "    for doc in data:\n",
    "        doc_list = [doc['docid'],doc['doctext']]\n",
    "        class_list = [0]*len(class_dict.keys())\n",
    "        for template in doc['templates']:\n",
    "            incident_type = template['incident_type']\n",
    "            if (incident_type=='bombing / attack') or (incident_type=='attack / bombing'):\n",
    "                class_list[class_dict['bombing']] = 1\n",
    "                class_list[class_dict['attack']] = 1\n",
    "                continue\n",
    "            class_list[class_dict[incident_type]] = 1\n",
    "        doc_list += class_list\n",
    "        class_data.loc[len(class_data)] = doc_list\n",
    "    return class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_class_data(get_data('../gtt_data/train.json'))\n",
    "dev_data = prepare_class_data(get_data('../gtt_data/dev.json'))\n",
    "test_data = prepare_class_data(get_data('../gtt_data/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data['Text']\n",
    "label_columns = train_data.columns.tolist()[2:]\n",
    "labels = train_data[label_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096', do_lower_case=True)\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  1886\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MucDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: LongformerTokenizer, max_token_len: int = 2000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        text = data_row['Text']\n",
    "        labels = data_row[label_columns]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return dict(text=text, input_ids=encoding[\"input_ids\"].flatten(),\n",
    "                    attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "                    labels=torch.FloatTensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MucDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, dev_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.dev_df = dev_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_dataset = MucDataset(self.train_df,\n",
    "                                        self.tokenizer,\n",
    "                                        self.max_token_len)\n",
    "        \n",
    "        self.dev_dataset = MucDataset(self.dev_df,\n",
    "                                      self.tokenizer,\n",
    "                                      self.max_token_len)\n",
    "\n",
    "        self.test_dataset = MucDataset(self.test_df,\n",
    "                                       self.tokenizer,\n",
    "                                       self.max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=12)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dev_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=12)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MucTagger(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.model = LongformerModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)    \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        for i, name in enumerate(label_columns):\n",
    "            class_f1 = f1(predictions[:, i], labels[:, i], num_classes=2)\n",
    "            self.logger.experiment.add_scalar(f\"{name}_f1/Train\", class_f1, self.current_epoch)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=self.n_warmup_steps,\n",
    "                                                    num_training_steps=self.n_training_steps)\n",
    "\n",
    "        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval='step'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "MODEL_NAME = 'allenai/longformer-base-4096'\n",
    "MAX_TOKEN_COUNT = 512\n",
    "\n",
    "steps_per_epoch=len(train_data) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = 20 #total_training_steps // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MucDataModule(train_data, dev_data, test_data, tokenizer, batch_size=BATCH_SIZE, \n",
    "                            max_token_len=MAX_TOKEN_COUNT)\n",
    "\n",
    "model = MucTagger(n_classes=len(label_columns), n_warmup_steps=warmup_steps,\n",
    "                  n_training_steps=total_training_steps)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints\", filename=\"best-checkpoint\",\n",
    "                                      save_top_k=1, verbose=True, monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"muc\")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(logger=logger, checkpoint_callback=checkpoint_callback,\n",
    "                     callbacks=[early_stopping_callback], max_epochs=N_EPOCHS, gpus=1,\n",
    "                     progress_bar_refresh_rate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | LongformerModel | 148 M \n",
      "1 | classifier | Linear          | 4.6 K \n",
      "2 | criterion  | BCELoss         | 0     \n",
      "-----------------------------------------------\n",
      "148 M     Trainable params\n",
      "0         Non-trainable params\n",
      "148 M     Total params\n",
      "594.656   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc6e856d9ad4072a773009e871e7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f57703ecc0479cbe425ca5aafac15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1299: val_loss reached 0.18524 (best 0.18524), saving model to \"/der/notebooks/checkpoints/best-checkpoint-v3.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = MucTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,\n",
    "                                               n_classes=len(label_columns))\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f62b3641ec42b2bae746b8150e0f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = MucDataset(\n",
    "  test_data,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    _, prediction = trained_model(item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "                                  item[\"attention_mask\"].unsqueeze(dim=0).to(device))\n",
    "    predictions.append(prediction.flatten())\n",
    "    labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Attack       0.87      0.60      0.71        89\n",
      "  Kidnapping       0.75      0.69      0.72        13\n",
      "     Bombing       0.83      0.83      0.83        48\n",
      "     Robbery       0.00      0.00      0.00         1\n",
      "       Arson       0.00      0.00      0.00         3\n",
      "      Forced       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.84      0.65      0.73       157\n",
      "   macro avg       0.41      0.35      0.38       157\n",
      "weighted avg       0.81      0.65      0.71       157\n",
      " samples avg       0.50      0.45      0.46       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > 0.5, upper, lower)\n",
    "\n",
    "print(classification_report(\n",
    "  y_true, \n",
    "  y_pred, \n",
    "  target_names=label_columns, \n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1116, 0.0171, 0.0329, 0.0147, 0.0167, 0.0097],\n",
      "        [0.8915, 0.0396, 0.0394, 0.0242, 0.0364, 0.0133],\n",
      "        [0.8710, 0.0360, 0.0302, 0.0202, 0.0297, 0.0109],\n",
      "        ...,\n",
      "        [0.2861, 0.3104, 0.0158, 0.0198, 0.0249, 0.0102],\n",
      "        [0.7578, 0.0227, 0.0258, 0.0150, 0.0213, 0.0086],\n",
      "        [0.2670, 0.0125, 0.0768, 0.0100, 0.0198, 0.0058]])\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
